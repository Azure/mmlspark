{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment with Spark Serving \nIn this example, we try to movie recommendations from the *Movie Ratings* dataset. Then we will use Spark serving to deploy it as a realtime web service. \nFirst, we import needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mmlspark\n",
    "import os\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read the data and split it to train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns\n",
    "userCol='UserId'\n",
    "itemCol='MovieId'\n",
    "ratingCol='Rating'\n",
    "\n",
    "# Download Movie Lens\n",
    "basedataurl = \"http://aka.ms\" \n",
    "datafile = \"MovieRatings.csv\"\n",
    "\n",
    "datafile_dbfs = os.path.join(\"/dbfs\", datafile)\n",
    "\n",
    "if os.path.isfile(datafile_dbfs):\n",
    "    print(\"found {} at {}\".format(datafile, datafile_dbfs))\n",
    "else:\n",
    "    print(\"downloading {} to {}\".format(datafile, datafile_dbfs))\n",
    "    urllib.request.urlretrieve(os.path.join(basedataurl, datafile), datafile_dbfs)\n",
    "    \n",
    "data_all = sqlContext.read.format('csv')\\\n",
    "                     .options(header='true', delimiter=',', inferSchema='true', ignoreLeadingWhiteSpace='true', ignoreTrailingWhiteSpace='true')\\\n",
    "                     .load(datafile).cache()    \n",
    "data_all.printSchema()\n",
    "display(data_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a Cross Validator pipeline, in order to tune a Spark ALS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmlspark.RankingTrainValidationSplit import RankingTrainValidationSplit\n",
    "from mmlspark.RankingEvaluator import RankingEvaluator\n",
    "\n",
    "als = ALS() \\\n",
    "    .setUserCol(userCol) \\\n",
    "    .setRatingCol(ratingCol) \\\n",
    "    .setItemCol(itemCol) \\\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(als.maxIter, [2, 4, 8]) \\\n",
    "    .addGrid(als.regParam, [1, 0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "itemCount = data_all.select(itemCol).distinct().count()\n",
    "evaluator = RankingEvaluator()\n",
    "\n",
    "rankingTrainValidationSplit = RankingTrainValidationSplit() \\\n",
    "    .setEstimator(als) \\\n",
    "    .setEvaluator(evaluator) \\\n",
    "    .setEstimatorParamMaps(paramGrid) \\\n",
    "    .setTrainRatio(0.8) \\\n",
    "    .setCollectSubMetrics(True)\n",
    "\n",
    "model = rankingTrainValidationSplit.fit(data_all)\n",
    "\n",
    "recommendations = model.bestModel.recommendForAllUsers(10).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define the webservice input/output.\nFor more information, you can visit the [documentation for Spark Serving](https://github.com/Azure/mmlspark/blob/master/docs/mmlspark-serving.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json, broadcast\n",
    "from pyspark.sql.types import *\n",
    "import uuid\n",
    "from mmlspark import request_to_string, string_to_response\n",
    "\n",
    "serving_inputs = spark.readStream.server() \\\n",
    "    .address(\"localhost\", 8898, \"my_api\") \\\n",
    "    .load()\\\n",
    "    .parseRequest(data_all.select(userCol).schema)\n",
    "\n",
    "serving_outputs = serving_inputs \\\n",
    "  .join(broadcast(recommendations), userCol) \\\n",
    "  .makeReply(\"recommendations\")\n",
    "\n",
    "server = serving_outputs.writeStream \\\n",
    "    .server() \\\n",
    "    .replyTo(\"my_api\") \\\n",
    "    .queryName(\"my_query\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoints-{}\".format(uuid.uuid1())) \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "data = u'{\"UserId\":1}'\n",
    "r = requests.post(data=data, url=\"http://localhost:8898/my_api\")\n",
    "print(\"Response {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "data = u'{\"UserId\":200}'\n",
    "r = requests.post(data=data, url=\"http://localhost:8898/my_api\")\n",
    "print(\"Response {}\".format(r.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(20) # wait for server to finish setting up (just to be safe)\n",
    "server.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": "107 - Model Deployment with Spark Serving"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
