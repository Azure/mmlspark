{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News Recommendation ALS Example Databricks Notebook\n",
    "##### by Daniel Ciborowski, dciborow@microsoft.com\n",
    "\n",
    "##### Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "##### Licensed under the MIT License.\n",
    "\n",
    "##### Setup\n",
    "1. Create new Cluster, Spark 3.0.1, Python3\n",
    "1. (Optional for Ranking Metrics) From Maven add to cluster the following jar: Azure:mmlspark:0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sample Data\n",
    "raw = [\n",
    "  {'userId': 1, 'itemId': 1, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 2, 'itemId': 1, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 3, 'itemId': 1, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 4, 'itemId': 1, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 5, 'itemId': 1, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 1, 'itemId': 2, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 2, 'itemId': 2, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 3, 'itemId': 2, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 4, 'itemId': 2, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 5, 'itemId': 2, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 1, 'itemId': 3, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 2, 'itemId': 3, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 3, 'itemId': 3, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 4, 'itemId': 3, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 5, 'itemId': 3, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 1, 'itemId': 4, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 2, 'itemId': 4, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 3, 'itemId': 4, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 4, 'itemId': 4, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 5, 'itemId': 4, 'rating':  random.randint(0, 10)},  \n",
    "  {'userId': 1, 'itemId': 5, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 2, 'itemId': 5, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 3, 'itemId': 5, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 4, 'itemId': 5, 'rating':  random.randint(0, 10)},\n",
    "  {'userId': 5, 'itemId': 5, 'rating':  random.randint(0, 10)},   \n",
    "]\n",
    "\n",
    "day1 = pd.DataFrame(raw)\n",
    "day2=pd.DataFrame(raw)\n",
    "day2['itemId'] = day2['itemId']+10\n",
    "day3=pd.DataFrame(raw)\n",
    "day3['itemId'] = day3['itemId']+20\n",
    "day4=pd.DataFrame(raw)\n",
    "day4['itemId'] = day4['itemId']+30\n",
    "\n",
    "data = day1 \\\n",
    "  .append(day2) \\\n",
    "  .append(day3) \\\n",
    "  .append(day4) \\\n",
    "  .sample(frac=0.75, replace=False)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "ratings = spark.createDataFrame(data)\n",
    "display(ratings.select('userId','itemId','rating').orderBy('userId','itemId'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS on the rating data\n",
    "# Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "algo = ALS(userCol=\"userId\", itemCol=\"itemId\", implicitPrefs=True, coldStartStrategy=\"drop\")\n",
    "model = algo.fit(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by computing the RMSE on the rating data\n",
    "predictions = model.transform(ratings)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model by computing ranking metrics on the rating data\n",
    "from mmlspark.recommendation.RankingAdapter import RankingAdapter\n",
    "from mmlspark.recommendation.RankingEvaluator import RankingEvaluator\n",
    "\n",
    "output = RankingAdapter(mode='allUsers', k=5, recommender=algo) \\\n",
    "  .fit(ratings) \\\n",
    "  .transform(ratings)\n",
    "\n",
    "metrics = ['ndcgAt','map','recallAtK','mrr','fcp']\n",
    "metrics_dict = {}\n",
    "for metric in metrics:\n",
    "    metrics_dict[metric] = RankingEvaluator(k=3, metricName=metric).evaluate(output)\n",
    "    \n",
    "metrics_dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend Subset Wrapper\n",
    "def recommendSubset(self, df):\n",
    "  def Func(lines):\n",
    "    out = []\n",
    "    for i in range(len(lines[1])):\n",
    "      out += [(lines[1][i],lines[2][i])]\n",
    "    return lines[0], out\n",
    "\n",
    "  tup = StructType([\n",
    "    StructField('itemId', IntegerType(), True),\n",
    "    StructField('rating', FloatType(), True)\n",
    "  ])\n",
    "  array_type = ArrayType(tup, True)\n",
    "\n",
    "  scoring = spark.createDataFrame(day4)\n",
    "  scored = self.transform(scoring)\n",
    "\n",
    "  recs = scored \\\n",
    "    .groupBy(col('userId')) \\\n",
    "    .agg(collect_list(col(\"itemId\")),collect_list(col(\"prediction\"))) \\\n",
    "    .rdd \\\n",
    "    .map(Func) \\\n",
    "    .toDF() \\\n",
    "    .withColumnRenamed(\"_1\",\"userId\") \\\n",
    "    .withColumnRenamed(\"_2\",\"recommendations\") \\\n",
    "    .select(col(\"userId\"),col(\"recommendations\").cast(array_type))\n",
    "\n",
    "  return recs\n",
    "\n",
    "import pyspark\n",
    "pyspark.ml.recommendation.ALSModel.recommendSubset = recommendSubset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Recommend most recent items for all users\n",
    "day4df = spark.createDataFrame(day4)\n",
    "recs = model.recommendSubset(day4df)\n",
    "\n",
    "display(recs.orderBy('userId'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
